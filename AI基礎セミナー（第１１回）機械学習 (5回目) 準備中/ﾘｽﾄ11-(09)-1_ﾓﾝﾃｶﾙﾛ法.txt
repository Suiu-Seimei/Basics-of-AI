#*************************************************************************
#  ﾘｽﾄ11-(09)-1_ﾓﾝﾃｶﾙﾛ法
#  ﾓﾝﾃｶﾙﾛ法で、
# 「(8.1) ﾓﾃﾞﾙﾌﾘｰな手法での例題」の「(例題) 崖歩き」の決定論的方策πを作成する
#*************************************************************************
import numpy as np
import time
import copy
import matplotlib.pyplot as plt
%matplotlib inline


#*************************************************************************
# ｸﾗｽ     Environment
#-------------------------------------------------------------------------
# 崖歩きの環境のｸﾗｽ
#*************************************************************************
class Environment:

  #***********************************************************************
  # Environment ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義する：
  #
  #    cliff_img    ：崖歩きの格子点情報 (縦4×横12)
  #                     ・'S'：開始点
  #                     ・'G'：目標点
  #                     ・'X'：崖領域の格子点
  #                     ・上記以外：歩行者が移動可能な格子点
  #-----------------------------------------------------------------------
  #    cliff        ：崖歩きの格子点情報を配列で表現
  #    size_y,size_x：崖歩きの格子点ﾃﾞｰﾀのｻｲｽﾞ（X方向,Y方向）
  #    states       ：状態値s（＝現在の位置座標(x,y)）の集合Ｓ
  #    state_ini    ：初期状態値（＝開始点の位置座標(x,y)）
  #
  #***********************************************************************
  def __init__(self):
    self.cliff_img = '''
++++++++++++
++++++++++++
++++++++++++
SXXXXXXXXXXG
'''

    # 崖歩きの格子点ﾃﾞｰﾀを、行上で配置情報を表す文字の配列を1行とし、行の配列で格納する
    #      'S':開始点、'G':目標点、'X':崖領域、'':左記以外の通行可能領域
    self.cliff = []
    for line in self.cliff_img.split('\n'):
      if line == '':
        continue 
      self.cliff.append(list(line))

    # 崖歩きの格子点ﾃﾞｰﾀのｻｲｽﾞ（X方向,Y方向）
    self.size_y, self.size_x = len(self.cliff), len(self.cliff[0])

    # 状態値s（＝現在の位置座標(x,y)）の集合Ｓをﾀﾌﾟﾙ型(x,y)の配列として保持する
    self.states = [(x, y) for x in range(self.size_x) for y in range(self.size_y)]
    
    # 初期状態値s0（＝開始点の位置座標(x,y)）ﾀﾌﾟﾙ型(x,y)として保持する
    for y in range(self.size_y):
      for x in range(self.size_x):
        if self.cliff[y][x] == 'S':
          self.state_ini = (x, y)
          break

  #***********************************************************************
  # Environment ｸﾗｽ : getCliff
  # 崖歩きの格子点情報を配列で取得するﾒｿｯﾄﾞ
  #-----------------------------------------------------------------------
  # 引数：なし
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) cliff : 崖歩きの格子点情報
  #***********************************************************************
  def getCliff(self):
    return self.cliff

  #***********************************************************************
  # Environment ｸﾗｽ : transit
  # 状態s で行動a を取った時の状態遷移に伴う
  # 報酬Ｒss'a、次の状態s'を返す。
  #-----------------------------------------------------------------------
  # 引数：
  #   s    ： 現在の状態s (＝現在の位置座標(x,y)）
  #   a    ： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) r  : 状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
  #                     0：目標点'G'への移動
  #                  -100：崖領域'X' への移動
  #                    -1：上記以外
  # (第２返値) s' : 状態s で行動a を取った時の遷移先の状態s'
  #                 (＝状態遷移後の位置座標(x',y')）)
  #***********************************************************************
  def transit(self, s, a):
        
    # 現在の状態s (＝位置座標(x,y)）
    x, y = s
    # 現在の状態s でとる行動a (＝移動量(Δx,Δy)）)
    dx, dy = a

    # 現在位置が出口の場合、状態・報酬共に不変
    if self.cliff[y][x] == 'G':
      return 0, s

    # 次状態へ遷移した場合、範囲外なら遷移せずに、報酬は -1
    if (y+dy < 0) or (y+dy >= self.size_y) :
        return -1, (x, y)
    if (x+dx < 0) or (x+dx >= self.size_x) :
        return -1, (x, y)

    # 次状態へ遷移した場合、崖領域'X' なら開始点'S'に戻る、報酬は -100
    if self.cliff[y+dy][x+dx] == 'X':
        x, y = self.state_ini
        return -100, (x, y)

    # 上記以外の場合、行動a のとおりに遷移を行う
    x += dx
    y += dy

    # 次状態へ遷移した場合、出口'G' なら、報酬は 0
    if self.cliff[y][x] == 'G':
        return 0, (x, y)

    # 上記以外なら、報酬は -1
    return -1, (x, y)


#*************************************************************************
# ｸﾗｽ     HyperParam
#-------------------------------------------------------------------------
# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀのｸﾗｽ
#*************************************************************************
class HyperParam:

  #***********************************************************************
  # HyperParam ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し、指定値で初期化する：
  #
  # eps_First： ｻﾝﾌﾟﾘﾝｸﾞ時のε-greedy法のｰﾊﾟﾗﾒｰﾀ (0≦ε≦1)εの初期値
  # eps_Last ： 学習時に、ε-greedy法のﾊﾟﾗﾒｰﾀεを徐々に減少させながら、
  #             ｴﾋﾟｿｰﾄﾞを繰り返して行動価値関数Ｑπ(s,a) の精度を上げてゆく。
  #             eps_Last は学習のためのｴﾋﾟｿｰﾄﾞ繰り返しの終了判断のεの最終値
  # eps_Rate ：εの1ｴﾋﾟｿｰﾄﾞ毎の減少率(0＜ε＜1、ε←⊿εrate×ε)
  # eps_Run  ：εの実行時の値
  #          （ε=0 で実行すると、方策πが完成していなくて終端に到達しない場合がある。
  #            εを 0 より大きいができるだけ小さい値で実行することにより、
  #            学習済の方策πを最大限に生かす方針の下で開始点から終了点まで移動する。)
  # trace    ：True:追跡出力する、False:しない
  #
  #***********************************************************************
  def __init__(self, eps_First, eps_Last, eps_Rate, eps_Run, trace):
    self.eps_First = eps_First
    self.eps_Last = eps_Last
    self.eps_Rate = eps_Rate
    self.eps_Run = eps_Run
    self.trace = trace


#*************************************************************************
# ｸﾗｽ     Agent
#-------------------------------------------------------------------------
# 崖歩きのｴｰｼﾞｪﾝﾄのｸﾗｽ
#*************************************************************************
class Agent:

  #***********************************************************************
  # Agent ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し初期化する：
  #
  #    env           ：Environment のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  #    actions       ：取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａ
  #    policy        ：決定論的方策π(s)（＝現在の状態s でとる行動a）
  #    q             ：状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
  #    n             ：状態s で行動a を取った訪問回数
  #
  #***********************************************************************
  def __init__(self, env):
    
    # 取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａをﾀﾌﾟﾙ型(Δx,Δy)の配列として保持する
    # (0,-1)：上, (-1,0)：左, (1,0)：右, (0,1)：下
    self.actions = [(0, -1), (-1, 0), (1, 0), (0, 1)]

    # 決定論的方策π を辞書型で保持し、乱数で初期化する
    #  キー：s、値：a
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    self.policy = {}
    for s in env.states:
      self.policy[s] = self.actions[np.random.randint(len(self.actions))]

    # 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) "q" を
    # ディクショナリ型で保持し、計算上あり得ない小さい値で初期化する 
    #  キー：(s,a)、値：q
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    #     q： 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
    self.q = {}
    for s in env.states:
      for a in self.actions:
        self.q[(s, a)] = -10**10

    # 状態s で行動a を取った訪問回数 "n" をディクショナリ型で保持し、0で初期化する 
    #  キー：(s,a)、値：n
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    #     n： 状態s で行動a を取った訪問回数Ｎ(s,a)
    self.n = {}
    for s in env.states:
      for a in self.actions:
        self.n[(s, a)] = 0

  #***********************************************************************
  # Agent ｸﾗｽ : getEpisode
  # ｼﾐｭﾚｰｼｮﾝにより、1回分のｴﾋﾟｿｰﾄﾞを取得する
  #-----------------------------------------------------------------------
  # 引数：
  #   env     ：Environment のｲﾝｽﾀﾝｽ
  #   epsilon ：ε-greedy法のﾊﾟﾗﾒｰﾀε
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 
  #             開始点から出口に至るまでの１回分のｴﾋﾟｿｰﾄﾞの
  #             状態遷移の流れを、ﾀﾌﾟﾙ型オブジェクト (s,a,r)  の
  #             ﾘｽﾄとして保存したもの
  #
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #          r：状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
  #***********************************************************************
  def getEpisode(self, env, epsilon):
    # １回分のｴﾋﾟｿｰﾄﾞの状態遷移の流れをﾘｽﾄで持つ
    episode = []

    # 状態s の初期値＝開始点
    s = env.state_ini

    # 「終端状態」となるまで繰り返す
    while True:
      if np.random.random() < epsilon:
        # εの割合で、ランダムに行動a を選択する
        a = self.actions[np.random.randint(len(agent.actions))]
      else:
        # 1-εの割合で、決定論的方策πに従って行動a を選択する
        a = self.policy[s]

      # 現在の状態s で、上記で選択した行動a を取り、状態遷移を行う
      r, s_dash = env.transit(s, a)
      
      # ｴﾋﾟｿｰﾄﾞへ状態遷移情報を追加する
      episode.append((s, a, r))
      
      # 状態遷移で終点に着いたら「終端状態」となるので、本ｴﾋﾟｿｰﾄﾞは終了する
      x, y = s_dash
      if env.cliff[y][x] == 'G':
        break

      # 状態s を更新
      s = s_dash

    return episode

  #***********************************************************************
  # Agent ｸﾗｽ : updatePolicy
  # 方策改善 (最適のものを１つだけ採用する)
  #-----------------------------------------------------------------------
  # 指定した状態s における最適行動価値関数Ｑ*(s,a) を求め(=q_max)、
  # その時の行動a* (=a_best)を、状態sにおける最適方策π*(s) として
  # 行動方策π(s) "policy[s]"を更新する。
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #-----------------------------------------------------------------------
  # 引数：
  #   s    ： 現在の状態s (＝位置座標(x,y))
  #***********************************************************************
  def updatePolicy(self, s):
    q_max = -10**10
    a_best = None
    for a in self.actions:
      if self.q[(s, a)] > q_max:
        q_max = self.q[(s, a)]
        a_best = a

    self.policy[s] = a_best

  #***********************************************************************
  # Agent ｸﾗｽ : train
  # ｼﾐｭﾚｰｼｮﾝにより、
  # ε-greedy法のﾊﾟﾗﾒｰﾀεを、ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの現象率で徐々に減少させ、
  # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの指定値より小さくなったら学習を終了するまで、
  # ｴﾋﾟｿｰﾄﾞを取得し、状態価値関数Ｖπ(s) を見積もる。
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode_lengths: 
  #      1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数を要素とするﾘｽﾄ型で保持し、その履歴とする
  #***********************************************************************
  def train(self, env, param):
    # 経過時間測定開始
    t1 = time.time()

    # 1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をﾘｽﾄ型で保持する
    episode_lengths = []

    # ε-greedy法のﾊﾟﾗﾒｰﾀεの初期化
    epsilon = param.eps_First

    # ε-greedy法のﾊﾟﾗﾒｰﾀεを徐々に減少させながら、
    # ｴﾋﾟｿｰﾄﾞの取得とそれによる方策改善を繰り返す
    seqNo = 0
    while True:
      seqNo += 1

      # 1回分のｴﾋﾟｿｰﾄﾞを取得
      episode = self.getEpisode(env, epsilon)

      # 1回分のｴﾋﾟｿｰﾄﾞでの状態遷移数を追加
      episode_lengths.append(len(episode))

      # 各状態s での決定論的方策π(s)のｵﾝﾎﾟﾘｼｰの範囲を確認するために
      # 終端から開始点に向かって参照できるように逆順にする
      episode.reverse()

      # 終端から開始点に向かって状態遷移情報を参照し、
      # 各状態s での決定論的方策π(s)のｵﾝﾎﾟﾘｼｰの範囲内の状態遷移情報で、
      # 以下の一連の処理で方策改善を行う

      # total_r は最終値は総報酬であるが、各状態s での総報酬にもなる
      total_r = 0
      
      last = False
      for (s, a, r) in episode:

        # 行動a が状態s での方策π(s)でなかったら
        # 本ｴﾋﾟｿｰﾄﾞによる方策改善で、残りの状態遷移を遡って参照することはしない。
        # 最後に参照する状態遷移情報は、
        # ε-greedy法によりεの割合で既存の方策πでない方策π'での行動a' 
        # を取ったことによる状態遷移情報
        if a != self.policy[s]:
          last = True

        # 各状態(s, a)での総報酬を更新
        total_r += r
      
        # 状態(s, a)への訪問回数を更新する
        self.n[(s, a)] += 1

        # 状態s の行動価値関数Ｑπ(s,a) を報酬Ｒss'a の総和の平均値として算出する
        self.q[(s, a)] += (total_r - self.q[(s, a)]) / self.n[(s, a)]

        # 状態s での方策を改善する
        self.updatePolicy(s)

        # 残りの状態遷移がｵﾝﾎﾟﾘｼｰの範囲内でなくなったら終了
        if last:
          break

      # ε-greedy法のﾊﾟﾗﾒｰﾀεを、ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの現象率で徐々に減少させ、
      # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの指定値より小さくなったら学習を終了する
      epsilon *= param.eps_Rate
      if epsilon < param.eps_Last:
        # 方策改善最後の1ｴﾋﾟｿｰﾄﾞ分の足跡を、表示する
        if(param.trace == True):
          print("\ntrain: last episode seqNo={0}".format(seqNo))
          showFootprint(self, env, episode)
        break

      # 1ｴﾋﾟｿｰﾄﾞ分の足跡を、100回ごとに表示する
      if(param.trace == True) and (seqNo % 100 == 0):
        print("\ntrain: episode seqNo={0}".format(seqNo))
        showFootprint(self, env, episode)

    # 経過時間を表示する
    t2 = time.time()
    print("学習経過時間：{0:.3f} [秒]".format(t2-t1))

    return episode_lengths

  #***********************************************************************
  # Agent ｸﾗｽ : run
  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路探索)を取得する
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 1ｴﾋﾟｿｰﾄﾞ分の状態遷移
  #***********************************************************************
  def run(self, env, param):

    # getEpisode 関数を、実行時のεで実行
    episode = self.getEpisode(env, param.eps_Run)
    return episode


#*************************************************************************
# showLearningCurve
# 方策πの学習で、1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をｸﾞﾗﾌ表示する
#-------------------------------------------------------------------------
# 引数：
#   episodeLengthHistory ：1ｴﾋﾟｿｰﾄﾞ分の状態遷移数の履歴
#*************************************************************************
def showLearningCurve(episodeLengthHistory):
  xlist = range(len(episodeLengthHistory))
  plt.figure(figsize=(12,3))
  plt.plot( xlist, episodeLengthHistory, 'cornflowerblue', linewidth=3 )
  plt.title("Transition count every episode")
  plt.xlabel("Episode no")
  plt.ylabel("count")
  plt.grid(True)
  plt.show()

#*************************************************************************
# showQ
# 行動価値関数Ｑπ(s,a)を表示する
#-------------------------------------------------------------------------
# 引数：
#   agent   ：Agent のｲﾝｽﾀﾝｽ
#   env     ：Environment のｲﾝｽﾀﾝｽ
#*************************************************************************
def showQ(agent, env):
  # 出力ﾏｯﾌﾟ
  maptbl = [['　' for x in range(env.size_x)] for y in range(env.size_y)]

  # 出力ﾏｯﾌﾟに写しこみ  
  for s in env.states:
    x, y = s
    aq_xy = ""
    for a in agent.actions:
      dx, dy = a
      q = agent.q[(s, a)]

      footprint = '　'
      if( dx == 0 and dy == -1 ):
        footprint = '↑'
      elif( dx == 0 and dy == 1 ):
        footprint = '↓'
      elif( dx == 1 and dy == 0 ):
        footprint = '→'
      elif( dx == -1 and dy == 0 ):
        footprint = '←'
      aq_xy = "{0}  {1}:{2:+7.2f}".format(aq_xy, footprint, q)
    maptbl[y][x] = aq_xy
        
  # 出力ﾏｯﾌﾟを表示  
  print("行動価値関数Ｑπ(s,a)")
  for y in range(env.size_y):
    print("Y={0:3}: ".format(y))
    for x in range(env.size_x):
       print("({0:3},{1:3}):{2}".format(x, y, maptbl[y][x]))
  print("")

#*************************************************************************
# showFootprint
# 1ｴﾋﾟｿｰﾄﾞ分の足跡を出力する
#-------------------------------------------------------------------------
# 引数：
#   agent   ：Agent のｲﾝｽﾀﾝｽ
#   env     ：Environment のｲﾝｽﾀﾝｽ
#   episode ：1ｴﾋﾟｿｰﾄﾞ分の状態遷移情報
#*************************************************************************
def showFootprint(agent, env, episode):

  # 状態遷移数
  print("ｴﾋﾟｿｰﾄﾞの状態遷移数：{0}".format(len(episode)))

  # 結果表示内容を初期化
  result = np.copy(env.cliff)
  for y in range(env.size_y):
    for x in range(env.size_x):
      if(result[y][x] == 'X' ):
        result[y][x] = '■'
      elif(result[y][x] == 'S' ):
        result[y][x] = 'Ｓ'
      elif(result[y][x] == 'G' ):
        result[y][x] = 'Ｇ'
      else:
        result[y][x] = '　'

  # 1ｴﾋﾟｿｰﾄﾞ分を取得する
  for (s, a, r) in episode:
    # 位置
    x, y = s
    
    # 方向
    dx, dy = a
    footprint = '＋'
    if( dx == 0 and dy == -1 ):
      footprint = '↑'
    elif( dx == 0 and dy == 1 ):
      footprint = '↓'
    elif( dx == 1 and dy == 0 ):
      footprint = '→'
    elif( dx == -1 and dy == 0 ):
      footprint = '←'

    # 格子上に足跡を残す
    result[y][x] = footprint

  #  1ｴﾋﾟｿｰﾄﾞ分の足跡を出力
  print("　＋", end="")
  for x in range(env.size_x):
    print("－", end="")
  print("＋")

  ycoord = 0
  for y in range(env.size_y):
    print("{0:2d}｜".format(ycoord), end="")
    for x in range(env.size_x):
      print(result[y][x], end="")
    print("｜")
    ycoord += 1
    
  print("　＋", end="")
  for x in range(env.size_x):
    print("－", end="")
  print("＋")
  print("　　", end="")
  for x in range(env.size_x):
    print("{0:2d}".format(x), end="")
  print()


#*************************************************************************
# 主制御
#*************************************************************************

#-------------------------------------------------------------------------
# (1) ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを設定
#-------------------------------------------------------------------------
eps_First = 0.9
eps_Last = 0.01
eps_Rate = 0.999
eps_Run = 0.001
param = HyperParam(eps_First, eps_Last, eps_Rate, eps_Run, False)

#-------------------------------------------------------------------------
# (2) 環境のｲﾝｽﾀﾝｽを作成
#-------------------------------------------------------------------------
env = Environment()

#-------------------------------------------------------------------------
# (3) 方策πの学習と利用を、100回繰り返す
#-------------------------------------------------------------------------
for rii in range(0,100):

  # Agent ｸﾗｽのｲﾝｽﾀﾝｽを作成する
  agent = Agent(env)

  # ﾓﾝﾃｶﾙﾛ法で方策πを学習する
  param.trace = False
  episode_lengths = agent.train(env, param)

  # 方策πの学習で、1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をｸﾞﾗﾌ表示する
  showLearningCurve(episode_lengths)

  # 方策πの学習終了時の、行動価値関数Ｑπ(s,a)を表示する
  showQ(agent, env)

  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路)を取得する
  param.trace = True
  episode = agent.run(env, param)

  # 1ｴﾋﾟｿｰﾄﾞ分の足跡を表示する
  showFootprint(agent, env, episode)

