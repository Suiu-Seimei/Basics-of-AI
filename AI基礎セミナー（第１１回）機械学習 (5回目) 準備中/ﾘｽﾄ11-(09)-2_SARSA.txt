#*************************************************************************
# ﾘｽﾄ 11-(09)-2_SARSA
#  SARSAで、
# 「(8.1) ﾓﾃﾞﾙﾌﾘｰな手法での例題」の「(例題) 崖歩き」の決定論的方策πを作成する
#*************************************************************************
import numpy as np
import time
import copy
import matplotlib.pyplot as plt
%matplotlib inline


#*************************************************************************
# ｸﾗｽ     Environment
#-------------------------------------------------------------------------
# 崖歩きの環境のｸﾗｽ
#-------------------------------------------------------------------------
# (※) 実装はﾓﾝﾃｶﾙﾛ法とおなじ
#*************************************************************************
class Environment:

  #***********************************************************************
  # Environment ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義する：
  #
  #    cliff_img    ：崖歩きの格子点情報 (縦4×横12)
  #                     ・'S'：開始点
  #                     ・'G'：目標点
  #                     ・'X'：崖領域の格子点
  #                     ・上記以外：歩行者が移動可能な格子点
  #-----------------------------------------------------------------------
  #    cliff        ：崖歩きの格子点情報を配列で表現
  #    size_y,size_x：崖歩きの格子点ﾃﾞｰﾀのｻｲｽﾞ（X方向,Y方向）
  #    states       ：状態値s（＝現在の位置座標(x,y)）の集合Ｓ
  #    state_ini    ：初期状態値（＝開始点の位置座標(x,y)）
  #
  #***********************************************************************
  def __init__(self):
    self.cliff_img = '''
++++++++++++
++++++++++++
++++++++++++
SXXXXXXXXXXG
'''

    # 崖歩きの格子点ﾃﾞｰﾀを、行上で配置情報を表す文字の配列を1行とし、行の配列で格納する
    #      'S':開始点、'G':目標点、'X':崖領域、'':左記以外の通行可能領域
    self.cliff = []
    for line in self.cliff_img.split('\n'):
      if line == '':
        continue 
      self.cliff.append(list(line))

    # 崖歩きの格子点ﾃﾞｰﾀのｻｲｽﾞ（X方向,Y方向）
    self.size_y, self.size_x = len(self.cliff), len(self.cliff[0])

    # 状態値s（＝現在の位置座標(x,y)）の集合Ｓをﾀﾌﾟﾙ型(x,y)の配列として保持する
    self.states = [(x, y) for x in range(self.size_x) for y in range(self.size_y)]
    
    # 初期状態値s0（＝開始点の位置座標(x,y)）ﾀﾌﾟﾙ型(x,y)として保持する
    for y in range(self.size_y):
      for x in range(self.size_x):
        if self.cliff[y][x] == 'S':
          self.state_ini = (x, y)
          break

  #***********************************************************************
  # Environment ｸﾗｽ : getCliff
  # 崖歩きの格子点情報を配列で取得するﾒｿｯﾄﾞ
  #-----------------------------------------------------------------------
  # 引数：なし
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) cliff : 崖歩きの格子点情報
  #***********************************************************************
  def getCliff(self):
    return self.cliff

  #***********************************************************************
  # Environment ｸﾗｽ : transit
  # 状態s で行動a を取った時の状態遷移に伴う
  # 報酬Ｒss'a、次の状態s'を返す。
  #-----------------------------------------------------------------------
  # 引数：
  #   s    ： 現在の状態s (＝現在の位置座標(x,y)）
  #   a    ： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) r  : 状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
  #                     0：目標点'G'への移動
  #                  -100：崖領域'X' への移動
  #                    -1：上記以外
  # (第２返値) s' : 状態s で行動a を取った時の遷移先の状態s'
  #                 (＝状態遷移後の位置座標(x',y')）)
  #***********************************************************************
  def transit(self, s, a):
        
    # 現在の状態s (＝位置座標(x,y)）
    x, y = s
    # 現在の状態s でとる行動a (＝移動量(Δx,Δy)）)
    dx, dy = a

    # 現在位置が出口の場合、状態・報酬共に不変
    if self.cliff[y][x] == 'G':
      return 0, s

    # 次状態へ遷移した場合、範囲外なら遷移せずに、報酬は -1
    if (y+dy < 0) or (y+dy >= self.size_y) :
        return -1, (x, y)
    if (x+dx < 0) or (x+dx >= self.size_x) :
        return -1, (x, y)

    # 次状態へ遷移した場合、崖領域'X' なら開始点'S'に戻る、報酬は -100
    if self.cliff[y+dy][x+dx] == 'X':
        x, y = self.state_ini
        return -100, (x, y)

    # 上記以外の場合、行動a のとおりに遷移を行う
    x += dx
    y += dy

    # 次状態へ遷移した場合、出口'G' なら、報酬は 0
    if self.cliff[y][x] == 'G':
        return 0, (x, y)

    # 上記以外なら、報酬は -1
    return -1, (x, y)


#*************************************************************************
# ｸﾗｽ     HyperParam
#-------------------------------------------------------------------------
# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀのｸﾗｽ
#-------------------------------------------------------------------------
# (※) 実装にﾓﾝﾃｶﾙﾛ法と相違あり：ﾊﾟﾗﾒｰﾀの構成に違いあり
#*************************************************************************
class HyperParam:

  #***********************************************************************
  # HyperParam ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し、指定値で初期化する：
  #
  # alpha      ：ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀα：係数(ｻﾝﾌﾟﾘﾝｸﾞ時の平均用の重み係数に相当)
  # gamma      ：ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀγ：割引率
  # eps_Train  ：ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀε：学習時のε-greedy法のﾊﾟﾗﾒｰﾀε
  # eps_Run    ：εの実行時の値
  #            （ε=0 で実行すると、方策πが完成していなくて終端に到達しない場合がある。
  #              εを 0 より大きいができるだけ小さい値で実行することにより、
  #              学習済の方策πを最大限に生かす方針の下で開始点から終了点まで移動する。)
  # episodeNum ：方策改善時に学習の為に取得するｴﾋﾟｿｰﾄﾞの最大回数
  # trace      ：True:追跡出力する、False:しない
  #
  #***********************************************************************
  def __init__(self, alpha, gamma, eps_Train, eps_Run, episodeNum, trace):
    # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀα、γ、ε
    self.alpha = alpha
    self.gamma = gamma
    self.eps_Train = eps_Train
    self.eps_Run = eps_Run
    self.episodeNum = episodeNum
    self.trace = trace


#*************************************************************************
# ｸﾗｽ     Agent
#-------------------------------------------------------------------------
# 崖歩きのｴｰｼﾞｪﾝﾄのｸﾗｽ
#*************************************************************************
class Agent:

  #***********************************************************************
  # Agent ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し初期化する：
  #
  #    env           ：Environment のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  #    actions       ：取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａ
  #    policy        ：決定論的方策π(s)（＝現在の状態s でとる行動a）
  #    q             ：状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
  #
  #-----------------------------------------------------------------------
  # (※) 実装にﾓﾝﾃｶﾙﾛ法と相違あり：
  #        ・訪問回数 "n" を持たない
  #***********************************************************************
  def __init__(self, env):
    
    # 取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａをﾀﾌﾟﾙ型(Δx,Δy)の配列として保持する
    # (0,-1)：上, (-1,0)：左, (1,0)：右, (0,1)：下
    self.actions = [(0, -1), (-1, 0), (1, 0), (0, 1)]

    # 決定論的方策π を辞書型で保持し、乱数で初期化する
    #  キー：s、値：a
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    self.policy = {}
    for s in env.states:
      self.policy[s] = self.actions[np.random.randint(len(self.actions))]

    # 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) "q" を
    # ディクショナリ型で保持し、0で初期化する 
    #  キー：(s,a)、値：q
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    #     q： 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
    self.q = {}
    for s in env.states:
      for a in self.actions:
        self.q[(s, a)] = 0

  #***********************************************************************
  # Agent ｸﾗｽ : getEpisodeAndLearn
  # ｼﾐｭﾚｰｼｮﾝにより、1回分のｴﾋﾟｿｰﾄﾞを取得しながら、方策改善を行う
  #-----------------------------------------------------------------------
  # 引数：
  #   env     ：Environment のｲﾝｽﾀﾝｽ
  #   param   ：HyperParam のｲﾝｽﾀﾝｽ
  #   epsilon ：ε-greedy法のﾊﾟﾗﾒｰﾀε (不使用)
  #   inTrain ：方策改善の為の学習中の場合:True、学習中でない場合:False
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 
  #             開始点から出口に至るまでの１回分のｴﾋﾟｿｰﾄﾞの
  #             状態遷移の流れを、ﾀﾌﾟﾙ型ｵﾌﾞｼﾞｪｸﾄ (s,a,r)  の
  #             ﾘｽﾄとして保存したもの
  #
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #          r：状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
  #***********************************************************************
  def getEpisodeAndLearn(self, env, param, epsilon, inTrain):
    # １回分のｴﾋﾟｿｰﾄﾞの状態遷移の流れをﾘｽﾄで持つ
    episode = []

    # 状態s の初期値＝開始点
    s = env.state_ini

    # 初期状態での行動a を方策πに従って選択する
    a = self.policy[s]
        
    # 「終端状態」となるまで繰り返す
    while True:
      
      # 現在の状態s で、選択した行動a を取り、状態遷移を行う
      r, s_dash = env.transit(s, a)

      # ｴﾋﾟｿｰﾄﾞへ状態遷移情報を追加する
      episode.append((s, a, r))

      # 遷移先の状態s' (s_dash) でとる行動a' (a_dash) を決定する
      a_dash = self.policy[s_dash]

      # 学習中の場合、
      # 行動価値関数Ｑπ(s,a)をＴＤ(0)法により推定して、
      # 方策πの改善を行う
      if inTrain:
        self.q[(s, a)] += param.alpha * (r + (param.gamma * self.q[(s_dash, a_dash)]) - self.q[(s, a)])
        self.updatePolicy(s)

      # 状態遷移で終点に着いたら「終端状態」となるので、本ｴﾋﾟｿｰﾄﾞは終了する
      x, y = s_dash
      if env.cliff[y][x] == 'G':
        break

      # 状態s と取る行動a を更新
      a = a_dash
      s = s_dash

    return episode

  #***********************************************************************
  # Agent ｸﾗｽ : updatePolicy
  # 方策改善 (最適のものを１つだけ採用する)
  #-----------------------------------------------------------------------
  # 指定した状態s における最適行動価値関数Ｑ*(s,a) を求め(=q_max)、
  # その時の行動a* (=a_best)を、状態sにおける最適方策π*(s) として
  # 行動方策π(s) "policy[s]"を更新する。
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #-----------------------------------------------------------------------
  # 引数：
  #   s    ： 現在の状態s (＝位置座標(x,y))
  #-----------------------------------------------------------------------
  # (※) 実装はﾓﾝﾃｶﾙﾛ法とおなじ
  #***********************************************************************
  def updatePolicy(self, s):
    q_max = -10**10
    a_best = None
    for a in self.actions:
      if self.q[(s, a)] > q_max:
        q_max = self.q[(s, a)]
        a_best = a

    self.policy[s] = a_best

  #***********************************************************************
  # Agent ｸﾗｽ : train
  # ｼﾐｭﾚｰｼｮﾝにより、
  # ｴﾋﾟｿｰﾄﾞ取得回数が指定回数 "episodeNum" に達するまで、
  # ｴﾋﾟｿｰﾄﾞを取得し、状態価値関数Ｖπ(s) を見積もる。
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode_lengths: 
  #      1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数を要素とするﾘｽﾄ型で保持し、その履歴とする
  #-----------------------------------------------------------------------
  # (※) 実装はﾓﾝﾃｶﾙﾛ法と全く異なる
  #***********************************************************************
  def train(self, env, param):
    # 経過時間測定開始
    t1 = time.time()

    # 指定回数分、ｴﾋﾟｿｰﾄﾞ取得と学習を繰り返す
    episode_lengths = []
    seqNo = 0
    for _ in range(param.episodeNum):
      seqNo += 1
      episode = self.getEpisodeAndLearn(env, param, param.eps_Train, True)
      episode_lengths.append(len(episode))
      
      # 1ｴﾋﾟｿｰﾄﾞ分の足跡を表示する
      if(param.trace == True):
        print("train: episode seqNo={0}".format(seqNo))
        showFootprint(self, env, episode)

    # 経過時間を表示する
    t2 = time.time()
    print("学習経過時間：{0:.3f} [秒]".format(t2-t1))

    return episode_lengths

  #***********************************************************************
  # Agent ｸﾗｽ : run
  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路探索)を取得する
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 1ｴﾋﾟｿｰﾄﾞ分の状態遷移
  #-----------------------------------------------------------------------
  # (※) 実装にﾓﾝﾃｶﾙﾛ法と相違あり
  #***********************************************************************
  def run(self, env, param):
    # getEpisodeAndLearn 関数を、ε=実行時の値、inTrain=False で実行
    episode = self.getEpisodeAndLearn(env, param, param.eps_Run, False)
    return episode


#*************************************************************************
# showLearningCurve
# 方策πの学習で、1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をｸﾞﾗﾌ表示する
#-------------------------------------------------------------------------
# 引数：
#   episodeLengthHistory ：1ｴﾋﾟｿｰﾄﾞ分の状態遷移数の履歴
#-------------------------------------------------------------------------
# (※) 実装はﾓﾝﾃｶﾙﾛ法とおなじ
#*************************************************************************
def showLearningCurve(episodeLengthHistory):
  xlist = range(len(episodeLengthHistory))
  plt.figure(figsize=(12,3))
  plt.plot( xlist, episodeLengthHistory, 'cornflowerblue', linewidth=3 )
  plt.title("Transition count every episode")
  plt.xlabel("Episode no")
  plt.ylabel("count")
  plt.grid(True)
  plt.show()

#*************************************************************************
# showQ
# 行動価値関数Ｑπ(s,a)を表示する
#-------------------------------------------------------------------------
# 引数：
#   agent   ：Agent のｲﾝｽﾀﾝｽ
#   env     ：Environment のｲﾝｽﾀﾝｽ
#-------------------------------------------------------------------------
# (※) 実装はﾓﾝﾃｶﾙﾛ法とおなじ
#*************************************************************************
def showQ(agent, env):
  # 出力ﾏｯﾌﾟ
  maptbl = [['　' for x in range(env.size_x)] for y in range(env.size_y)]

  # 出力ﾏｯﾌﾟに写しこみ  
  for s in env.states:
    x, y = s
    aq_xy = ""
    for a in agent.actions:
      dx, dy = a
      q = agent.q[(s, a)]

      footprint = '　'
      if( dx == 0 and dy == -1 ):
        footprint = '↑'
      elif( dx == 0 and dy == 1 ):
        footprint = '↓'
      elif( dx == 1 and dy == 0 ):
        footprint = '→'
      elif( dx == -1 and dy == 0 ):
        footprint = '←'
      aq_xy = "{0}  {1}:{2:+7.2f}".format(aq_xy, footprint, q)
    maptbl[y][x] = aq_xy
        
  # 出力ﾏｯﾌﾟを表示  
  print("行動価値関数Ｑπ(s,a)")
  for y in range(env.size_y):
    print("Y={0:3}: ".format(y))
    for x in range(env.size_x):
       print("({0:3},{1:3}):{2}".format(x, y, maptbl[y][x]))
  print("")

#*************************************************************************
# showFootprint
# 1ｴﾋﾟｿｰﾄﾞ分の足跡を出力する
#-------------------------------------------------------------------------
# 引数：
#   agent   ：Agent のｲﾝｽﾀﾝｽ
#   env     ：Environment のｲﾝｽﾀﾝｽ
#   episode ：1ｴﾋﾟｿｰﾄﾞ分の状態遷移情報
#-------------------------------------------------------------------------
# (※) 実装はﾓﾝﾃｶﾙﾛ法とおなじ
#*************************************************************************
def showFootprint(agent, env, episode):

  # 状態遷移数
  print("ｴﾋﾟｿｰﾄﾞの状態遷移数：{0}".format(len(episode)))

  # 結果表示内容を初期化
  result = np.copy(env.cliff)
  for y in range(env.size_y):
    for x in range(env.size_x):
      if(result[y][x] == 'X' ):
        result[y][x] = '■'
      elif(result[y][x] == 'S' ):
        result[y][x] = 'Ｓ'
      elif(result[y][x] == 'G' ):
        result[y][x] = 'Ｇ'
      else:
        result[y][x] = '　'

  # 1ｴﾋﾟｿｰﾄﾞ分を取得する
  for (s, a, r) in episode:
    # 位置
    x, y = s
    
    # 方向
    dx, dy = a
    footprint = '＋'
    if( dx == 0 and dy == -1 ):
      footprint = '↑'
    elif( dx == 0 and dy == 1 ):
      footprint = '↓'
    elif( dx == 1 and dy == 0 ):
      footprint = '→'
    elif( dx == -1 and dy == 0 ):
      footprint = '←'

    # 格子上に足跡を残す
    result[y][x] = footprint

  #  1ｴﾋﾟｿｰﾄﾞ分の足跡を出力
  print("　＋", end="")
  for x in range(env.size_x):
    print("−", end="")
  print("＋")

  ycoord = 0
  for y in range(env.size_y):
    print("{0:2d}｜".format(ycoord), end="")
    for x in range(env.size_x):
      print(result[y][x], end="")
    print("｜")
    ycoord += 1
    
  print("　＋", end="")
  for x in range(env.size_x):
    print("−", end="")
  print("＋")
  print("　　", end="")
  for x in range(env.size_x):
    print("{0:2d}".format(x), end="")
  print()


#*************************************************************************
# 主制御
#*************************************************************************

#-------------------------------------------------------------------------
# (1) ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを設定
#-------------------------------------------------------------------------
alpha = 0.2
gamma = 1
eps_Train = 0.1
eps_Run = 0.001
episodeNum = 4500
param = HyperParam(alpha, gamma, eps_Train, eps_Run, episodeNum, False)

#-------------------------------------------------------------------------
# (2) 環境のｲﾝｽﾀﾝｽを作成
#-------------------------------------------------------------------------
env = Environment()

#-------------------------------------------------------------------------
# (3) 方策πの学習と利用を、100回繰り返す
#-------------------------------------------------------------------------
for rii in range(0,100):

  # Agent ｸﾗｽのｲﾝｽﾀﾝｽを作成する
  agent = Agent(env)

  # SARSA で方策πを学習する
  param.trace = False
  episode_lengths = agent.train(env, param)

  # 方策πの学習で、1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をｸﾞﾗﾌ表示する
  showLearningCurve(episode_lengths)

  # 方策πの学習終了時の、行動価値関数Ｑπ(s,a)を表示する
  showQ(agent, env)

  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路)を取得する
  param.trace = True
  episode = agent.run(env, param)

  # 1ｴﾋﾟｿｰﾄﾞ分の足跡を表示する
  showFootprint(agent, env, episode)

