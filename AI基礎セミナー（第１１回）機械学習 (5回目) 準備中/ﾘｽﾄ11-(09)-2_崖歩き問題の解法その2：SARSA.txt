#*************************************************************************
# ﾘｽﾄ11-(09)-2_崖歩き問題の解法その2：SARSA
#  SARSAで、
# 「(8.1) ﾓﾃﾞﾙﾌﾘｰな手法での例題」の「(例題) 崖歩き」の決定論的方策πを作成する。
#  ※ 事前に「ﾘｽﾄ11-(08)-1_崖歩き問題の環境と表示のｸﾗｽ」を実行しておくこと。
#*************************************************************************
#import numpy as np
import time
#import matplotlib.pyplot as plt


#*************************************************************************
# ｸﾗｽ     HyperParam
#-------------------------------------------------------------------------
# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀのｸﾗｽ
#-------------------------------------------------------------------------
# (※) 実装にﾓﾝﾃｶﾙﾛ法と相違あり：ﾊﾟﾗﾒｰﾀの構成に違いあり
#*************************************************************************
class HyperParam:

  #***********************************************************************
  # HyperParam ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し、指定値で初期化する：
  #
  # alpha      ：ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀα：係数(ｻﾝﾌﾟﾘﾝｸﾞ時の平均用の重み係数に相当)
  # gamma      ：ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀγ：割引率
  # eps_Train  ：ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀε：学習時のε-greedy法のﾊﾟﾗﾒｰﾀε
  # eps_Run    ：εの実行時の値
  #            （ε=0 で実行すると、方策πが完成していなくて終端に到達しない場合がある。
  #              εを 0 より大きいができるだけ小さい値で実行することにより、
  #              学習済の方策πを最大限に生かす方針の下で開始点から終了点まで移動する。)
  # episodeNum ：方策改善時に学習の為に取得するｴﾋﾟｿｰﾄﾞの最大回数
  # trace      ：True:追跡出力する、False:しない
  #
  #***********************************************************************
  def __init__(self, alpha, gamma, eps_Train, eps_Run, episodeNum, trace):
    # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀα、γ、ε
    self.alpha = alpha
    self.gamma = gamma
    self.eps_Train = eps_Train
    self.eps_Run = eps_Run
    self.episodeNum = episodeNum
    self.trace = trace


#*************************************************************************
# ｸﾗｽ     Agent
#-------------------------------------------------------------------------
# 崖歩きのｴｰｼﾞｪﾝﾄのｸﾗｽ
#*************************************************************************
class Agent:

  #***********************************************************************
  # Agent ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し初期化する：
  #
  #    env           ：Environment のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  #    actions       ：取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａ
  #    policy        ：決定論的方策π(s)（＝現在の状態s でとる行動a）
  #    q             ：状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
  #
  #-----------------------------------------------------------------------
  # (※) 実装にﾓﾝﾃｶﾙﾛ法と相違あり：
  #        ・訪問回数 "n" を持たない
  #***********************************************************************
  def __init__(self, env):
    
    # 取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａをﾀﾌﾟﾙ型(Δx,Δy)の配列として保持する
    # (0,-1)：上, (-1,0)：左, (1,0)：右, (0,1)：下
    self.actions = [(0, -1), (-1, 0), (1, 0), (0, 1)]

    # 決定論的方策π を辞書型で保持し、乱数で初期化する
    #  キー：s、値：a
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    self.policy = {}
    for s in env.states:
      self.policy[s] = self.actions[np.random.randint(len(self.actions))]

    # 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) "q" を
    # ディクショナリ型で保持し、0で初期化する 
    #  キー：(s,a)、値：q
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    #     q： 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
    self.q = {}
    for s in env.states:
      for a in self.actions:
        self.q[(s, a)] = 0

  #***********************************************************************
  # Agent ｸﾗｽ : getEpisodeAndLearn
  # ｼﾐｭﾚｰｼｮﾝにより、1回分のｴﾋﾟｿｰﾄﾞを取得しながら、方策改善を行う
  #-----------------------------------------------------------------------
  # 引数：
  #   env     ：Environment のｲﾝｽﾀﾝｽ
  #   param   ：HyperParam のｲﾝｽﾀﾝｽ
  #   epsilon ：ε-greedy法のﾊﾟﾗﾒｰﾀε (不使用)
  #   inTrain ：方策改善の為の学習中の場合:True、学習中でない場合:False
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 
  #             開始点から出口に至るまでの１回分のｴﾋﾟｿｰﾄﾞの
  #             状態遷移の流れを、ﾀﾌﾟﾙ型ｵﾌﾞｼﾞｪｸﾄ (s,a,r)  の
  #             ﾘｽﾄとして保存したもの
  #
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #          r：状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
  #***********************************************************************
  def getEpisodeAndLearn(self, env, param, epsilon, inTrain):
    # １回分のｴﾋﾟｿｰﾄﾞの状態遷移の流れをﾘｽﾄで持つ
    episode = []

    # 状態s の初期値＝開始点
    s = env.state_ini

    # 初期状態での行動a を方策πに従って選択する
    a = self.policy[s]
        
    # 「終端状態」となるまで繰り返す
    while True:
      
      # 現在の状態s で、選択した行動a を取り、状態遷移を行う
      r, s_dash = env.transit(s, a)

      # ｴﾋﾟｿｰﾄﾞへ状態遷移情報を追加する
      episode.append((s, a, r))

      # 遷移先の状態s' (s_dash) でとる行動a' (a_dash) を決定する
      a_dash = self.policy[s_dash]

      # 学習中の場合、
      # 行動価値関数Ｑπ(s,a)をＴＤ(0)法により推定して、
      # 方策πの改善を行う
      if inTrain:
        self.q[(s, a)] += param.alpha * (r + (param.gamma * self.q[(s_dash, a_dash)]) - self.q[(s, a)])
        self.updatePolicy(s)

      # 状態遷移で終点に着いたら「終端状態」となるので、本ｴﾋﾟｿｰﾄﾞは終了する
      x, y = s_dash
      if env.cliff[y][x] == 'G':
        break

      # 状態s と取る行動a を更新
      a = a_dash
      s = s_dash

    return episode

  #***********************************************************************
  # Agent ｸﾗｽ : updatePolicy
  # 方策改善 (最適のものを１つだけ採用する)
  #-----------------------------------------------------------------------
  # 指定した状態s における最適行動価値関数Ｑ*(s,a) を求め(=q_max)、
  # その時の行動a* (=a_best)を、状態sにおける最適方策π*(s) として
  # 行動方策π(s) "policy[s]"を更新する。
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #-----------------------------------------------------------------------
  # 引数：
  #   s    ： 現在の状態s (＝位置座標(x,y))
  #-----------------------------------------------------------------------
  # (※) 実装はﾓﾝﾃｶﾙﾛ法とおなじ
  #***********************************************************************
  def updatePolicy(self, s):
    q_max = -10**10
    a_best = None
    for a in self.actions:
      if self.q[(s, a)] > q_max:
        q_max = self.q[(s, a)]
        a_best = a

    self.policy[s] = a_best

  #***********************************************************************
  # Agent ｸﾗｽ : train
  # ｼﾐｭﾚｰｼｮﾝにより、
  # ｴﾋﾟｿｰﾄﾞ取得回数が指定回数 "episodeNum" に達するまで、
  # ｴﾋﾟｿｰﾄﾞを取得し、状態価値関数Ｖπ(s) を見積もる。
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode_lengths: 
  #      1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数を要素とするﾘｽﾄ型で保持し、その履歴とする
  #-----------------------------------------------------------------------
  # (※) 実装はﾓﾝﾃｶﾙﾛ法と全く異なる
  #***********************************************************************
  def train(self, env, param):
    # 経過時間測定開始
    t1 = time.time()

    # 指定回数分、ｴﾋﾟｿｰﾄﾞ取得と学習を繰り返す
    episode_lengths = []
    seqNo = 0
    for _ in range(param.episodeNum):
      seqNo += 1
      episode = self.getEpisodeAndLearn(env, param, param.eps_Train, True)
      episode_lengths.append(len(episode))
      
      # 1ｴﾋﾟｿｰﾄﾞ分の足跡を表示する
      if(param.trace == True):
        print("train: episode seqNo={0}".format(seqNo))
        showFootprint(self, env, episode)

    # 経過時間を表示する
    t2 = time.time()
    print("学習経過時間：{0:.3f} [秒]".format(t2-t1))

    return episode_lengths

  #***********************************************************************
  # Agent ｸﾗｽ : run
  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路探索)を取得する
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 1ｴﾋﾟｿｰﾄﾞ分の状態遷移
  #-----------------------------------------------------------------------
  # (※) 実装にﾓﾝﾃｶﾙﾛ法と相違あり
  #***********************************************************************
  def run(self, env, param):
    # getEpisodeAndLearn 関数を、ε=実行時の値、inTrain=False で実行
    episode = self.getEpisodeAndLearn(env, param, param.eps_Run, False)
    return episode


#*************************************************************************
# 主制御
#*************************************************************************

#-------------------------------------------------------------------------
# (1) ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを設定
#-------------------------------------------------------------------------
alpha = 0.2
gamma = 1
eps_Train = 0.1
eps_Run = 0.001
episodeNum = 4500
param = HyperParam(alpha, gamma, eps_Train, eps_Run, episodeNum, False)

#-------------------------------------------------------------------------
# (2) 環境のｲﾝｽﾀﾝｽを作成
#-------------------------------------------------------------------------
env = Environment()

#-------------------------------------------------------------------------
# (3) 方策πの学習と利用を、100回繰り返す
#-------------------------------------------------------------------------
showObj = ShowFunc()
for rii in range(0,100):

  # Agent ｸﾗｽのｲﾝｽﾀﾝｽを作成する
  agent = Agent(env)

  # SARSA で方策πを学習する
  param.trace = False
  episode_lengths = agent.train(env, param)

  # 方策πの学習で、1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をｸﾞﾗﾌ表示する
  showObj.showLearningCurve(episode_lengths)

  # 方策πの学習終了時の、行動価値関数Ｑπ(s,a)を表示する
  showObj.showQ(agent, env)

  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路)を取得する
  param.trace = True
  episode = agent.run(env, param)

  # 1ｴﾋﾟｿｰﾄﾞ分の足跡を表示する
  showObj.showFootprint(agent, env, episode)
