#*************************************************************************
# ﾘｽﾄ11-(07)-1_方策反復法 (方策評価と、ε-greedy法による方策改善)
#*************************************************************************
import sys
import numpy as np

#*************************************************************************
# ｸﾗｽ     Environment
#-------------------------------------------------------------------------
# 環境のｸﾗｽ
#*************************************************************************
class Environment:

    #*********************************************************************
    # Environment ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
    #---------------------------------------------------------------------
    # 以下のｲﾝｽﾀﾝｽ変数を定義する：
    #    states       ：状態値s（＝現在の位置座標(x,y)）の集合Ｓ
    #*********************************************************************
    def __init__(self):

        # 状態s の集合Ｓを、状態名を表す文字列のリストで作成
        self.states = ["s1", "s2"]

    #*********************************************************************
    # Environment ｸﾗｽ : transit
    # 状態s で行動a を取った時の状態遷移に伴う
    # （遷移確率Ｐss'a、報酬Ｒss'a、次の状態s'）のタプル型オブジェクトを
    # ﾘｽﾄで返す（これは(表3.2)を実装したもの）。
    #---------------------------------------------------------------------
    # 引数：
    #   self  ：Agent のｲﾝｽﾀﾝｽ
    #   s     ：現在の状態s
    #   a     ：状態s でとる行動a
    #---------------------------------------------------------------------
    # 戻り値：
    # (第１返値) 以下の値の組のタプル型ｵﾌﾞｼﾞｪｸﾄ(p,r,s) の配列
    #      p   : 状態s で行動a を取った時の状態s'への遷移確率Ｐss'a
    #      r   : 状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
    #      s'  : 状態s で行動a を取った時の遷移先の状態s'
    #*********************************************************************
    def transit(self, s, a):

        if( s == "s1" ):
            if( a == "a1" ):
                return [ (0.3,  1.0, "s1"), (0.7, -2.0, "s2") ]
            elif( a == "a2" ):
                return [ (0.5,  1.0, "s1"), (0.5,  2.0, "s2") ]

        elif( s == "s2" ):
            if( a == "a1" ):
                return [ (0.6, -1.0, "s1"), (0.4,  2.0, "s2") ]
            elif( a == "a2" ):
                return [ (0.7,  1.0, "s1"), (0.3,  1.0, "s2") ]

        return [ (1.0,  0.0,   s ) ]


#*************************************************************************
# ｸﾗｽ     HyperParam
#-------------------------------------------------------------------------
# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀのｸﾗｽ
#*************************************************************************
class HyperParam:

    #*********************************************************************
    # HyperParam ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
    #---------------------------------------------------------------------
    # 以下のｲﾝｽﾀﾝｽ変数を定義し、指定値で初期化する：
    #
    #    epsilon      ：ε-greedy法のパラメータε
    #    gamma        ：割引率γ (0≦γ＜1)
    #    delta_thresh ：状態価値関数Ｖπ(s) の更新量のしきい値
    #              (これより更新量が小さければ、状態価値関数の更新を終了する)
    #
    #*********************************************************************
    def __init__(self, epsilon, gamma, delta_thresh ):

        # epsilon：ε-greedy法のパラメータε
        self.epsilon = epsilon

        # gamma：割引率γ
        self.gamma = gamma

        # delta_thresh ：状態価値関数Ｖπ(s) の更新量のしきい値
        #              （これより更新量が小さければ更新を終了する）
        self.delta_thresh = delta_thresh

    #*********************************************************************
    # HyperParam ｸﾗｽ : showHyperParameters
    # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを出力する。
    #---------------------------------------------------------------------
    # 引数：なし
    #*********************************************************************
    def showHyperParameters(self):
        print( "Hyper parameters :")
        print( "  (1) ε={0:.3f}".format(self.epsilon) )
        print( "  (2) γ={0:.3f}".format(self.gamma) )
        print( "  (3) delta_thresh={0}".format(self.delta_thresh) )


#*************************************************************************
# クラス     Agent
#-------------------------------------------------------------------------
# ｴｰｼﾞｪﾝﾄのｸﾗｽ
# ε-greedy法を用いた方策反復法による方策改善の (表3.2) への適用
#*************************************************************************
class Agent:

    #*********************************************************************
    # Agent ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
    #---------------------------------------------------------------------
    # 以下のｲﾝｽﾀﾝｽ変数を定義し初期化する：
    #
    #   actions ：行動aの集合Ａを、行動名を表す文字列のリストで作成
    #   pai     ：方策πを辞書型で作成
    #                ｷｰ：(s,a)、ﾊﾞﾘｭｰ：π(a|s)
    #                方策の初期値：行動 "a1"を50％、"a2"を50％の割合で選択
    #   v_pais  ：状態s の状態価値関数Ｖπ(s)を辞書型で作成し、0で初期化
    #                ｷｰ：s(状態名を表す文字列型)、ﾊﾞﾘｭｰ：Ｖπ(s)
    #   q_paisa ：状態s での行動a の行動価値関数Ｑπ(s,a)を辞書型で作成し
    #             0で初期化
    #                ｷｰ：(s,a) のタプル型、ﾊﾞﾘｭｰ：Ｑπ(s,a)
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def __init__(self, env):

        # 行動aの集合Ａを、行動名を表す文字列のリストで作成
        self.actions = ["a1", "a2"]

        # 方策πを辞書型で作成
        #     ｷｰ：(s,a)、ﾊﾞﾘｭｰ：π(a|s)
        #     方策の初期値：行動 "a1" と "a2" を平等の割合で選択する
        self.pai = {}
        for s in env.states:
            for a in self.actions:
                self.pai[(s, a)] = 1.0/len(self.actions)

        # 状態(=位置)の状態価値関数Ｖπ(s)を辞書型で作成し、0で初期化
        # ｷｰ：s(位置：2次元座標を表すタプル型)、ﾊﾞﾘｭｰ：Ｖπ(s)
        self.v_pais = {}
        for s in env.states:
            self.v_pais[s] = 0

        # 状態s での行動a 行動価値関数Ｑπ(s,a)を辞書型で作成し、0で初期化
        # ｷｰ：(s,a) のタプル型、ﾊﾞﾘｭｰ：Ｑπ(s,a)
        self.q_paisa = {}
        for s in env.states:
            for a in self.actions:
                self.q_paisa[(s,a)] = 0

    #*********************************************************************
    # Agent ｸﾗｽ : evaluatePolicy
    # Agent のｲﾝｽﾀﾝｽに登録済の方策πの下での、
    # 状態価値関数Ｖπ(s)を算出する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #   param   ：HyperParam のｲﾝｽﾀﾝｽ
    #---------------------------------------------------------------------
    # 戻り値：    なし
    #*********************************************************************
    def evaluatePolicy(self, env, param):

        # 状態価値関数Ｖπ(s) の値が収束するまで繰り返す
        repeatNo = 0
        while True:
            repeatNo += 1
            
            # 状態価値関数Ｖπ(s) の更新量の最大値を初期化
            delta_max = 0

            # 全ての状態s について繰り返す
            for s in env.states:

                # 状態s の状態価値関数Ｖπ(s)を初期化する
                # v_pais_dash＝Ｖπ(s)＝Σa π(a|s) Σs'Ｐss'a×｛Ｒss'a＋γＶπ(s')｝
                v_pais_dash = 0

                # 全ての行動a について繰り返す
                for a in self.actions:
                    
                    # 状態s で行動a を取った時の状態遷移に伴う (p,r,s_dash) をタプル型オブジェクトで持ち、
                    # その組をリスト(results) として取得する
                    #     p＝Ｐss'a：状態がs の時に、行動a を選択して状態がs'に遷移する確率
                    #     r＝Ｒss'a：状態がs の時に、行動a を選択して状態がs'に遷移した時の報酬
                    #     s_dash＝s'：遷移先の状態s'
                    results = env.transit(s, a)
                    
                    # ﾍﾞﾙﾏﾝ方程式で、状態価値関数Ｖπ(s) の行動a による寄与分を計算する
                    # （行動a による寄与分＝π(a|s) Σs'Ｐss'a×｛Ｒss'a＋γＶπ(s')｝ ）
                    # （※ 次式で参照する agent.v_pais[s_dash] の設定値は、
                    #      状態価値関数Ｖπ(s)の計算を繰り返し行うことで収束する）
                    for p, r, s_dash in results:
                        v_pais_dash += self.pai[(s, a)] * p * (r + param.gamma * self.v_pais[s_dash])

                # 状態価値関数Ｖπ(s) の更新量の最大値を保存
                delta_max = max(delta_max, abs(self.v_pais[s] - v_pais_dash))

                # 状態s の状態価値関数Ｖπ(s)を更新
                self.v_pais[s] = v_pais_dash

            # 状態価値関数Ｖπ(s) の更新量の最大値(delta_max)がしきい値(delta_thresh)より小さければ
            # 状態価値関数Ｖπ(s) の値が収束したものとして、繰り返し計算を終了
            if delta_max < param.delta_thresh:
                print("  evaluatePolicy: repeated {0} times".format(repeatNo))
                break

    #*********************************************************************
    # Agent ｸﾗｽ : updatePolicy
    # 方策改善を ε-greedy法で行う。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #   param   ：HyperParam のｲﾝｽﾀﾝｽ
    #---------------------------------------------------------------------
    # 戻り値：
    # (第１返値) updateFlag :方策確率πの更新有無フラグ
    #                       (True：更新がある、False：更新がない)
    #*********************************************************************
    def updatePolicy(self, env, param):
        # 方策確率πの更新有無フラグ
        updateFlag = False

        # 全ての状態s について繰り返す    
        for s in env.states:

            # 最適行動価値関数Ｑ*(s,a) を初期化
            q_max = -sys.maxsize -1

            # 最適行動価値関数Ｑ*(s,a)を与える行動a* の集合Ａ*(s)をリストで作成
            a_asters = []

            # 全ての行動a について繰り返す
            for a in self.actions:

                # 状態s で行動a を取った時の状態遷移に伴う (p,r,s_dash) をタプル型オブジェクトで持ち、
                # その組をリスト(results) として取得する
                #     p＝Ｐss'a：状態がs の時に、行動a を選択して状態がs'に遷移する確率
                #     r＝Ｒss'a：状態がs の時に、行動a を選択して状態がs'に遷移した時の報酬
                #     s_dash＝s'：遷移先の状態s'
                results = env.transit(s, a)

                # 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) を計算する
                # q_paisa＝Ｑπ(s,a)＝Σs'Ｐss'a×｛Ｒss'a＋γＶπ(s')｝
                q_paisa = 0
                for p, r, s_dash in results:
                    q_paisa += p * (r + param.gamma * self.v_pais[s_dash])
                self.q_paisa[(s,a)] = q_paisa

                # 状態s での最適行動価値関数Ｑ*(s,a)を更新し、
                # その時の行動a* も記録する。
                if q_paisa > q_max:
                    q_max = q_paisa
                    a_asters = [a]
                elif q_paisa == q_max:
                    a_asters.append(a)

            # M ：集合Ａ(s)の要素数
            # M*：集合Ａ*(s)の要素数
            M = len(self.actions)
            M_aster = len(a_asters)
            pai_aster = (1-param.epsilon)/M_aster + (param.epsilon / M)
            pai_a     = param.epsilon / M

            # (ε-greedy法による方策改善)
            # 着目している状態s での
            # (1) 行動a*            の方策確率π(a*|s)を (1-ε)/M*＋ε/M
            # (2) 他の行動a (a≠a*) の方策確率π(a |s)を            ε/M
            # に各々更新する。
            for a in self.actions:
                if( a in a_asters ):
                    if( self.pai[(s, a)] != pai_aster ):
                        updateFlag = True
                        self.pai[(s, a)] = pai_aster 
                else:
                    if( self.pai[(s, a)] != pai_a ):
                        updateFlag = True
                        self.pai[(s, a)] = pai_a

        # 方策確率πの更新有無フラグを返す
        return updateFlag

    #*********************************************************************
    # Agent ｸﾗｽ : iteratePolicy
    # 方策反復法による方策改善を行う。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #   param   ：HyperParam のｲﾝｽﾀﾝｽ
    #---------------------------------------------------------------------
    # 戻り値：    なし
    #*********************************************************************
    def iteratePolicy(self, env, param):
        iterateNo = 0
        while True:
            iterateNo += 1
            print( "\n【 iteratePolicy : No.={0} 】".format(iterateNo) )

            # 方策π(a|s) を出力
            self.showPolicy(env)

            # 方策πの下での、状態価値関数Ｖπ(s)を算出
            self.evaluatePolicy(env, param)

            # 状態価値関数Ｖπ(s) を出力
            self.showStateValues(env)

            # 方策改善を行う
            updateFlag = self.updatePolicy(env, param)

            # 行動価値関数Ｑπ(s,a) を出力する。
            self.showActionValues(env)

            # 方策に更新が無ければ繰り返し終了
            if not updateFlag:
                break

    #*********************************************************************
    # Agent ｸﾗｽ : showPolicy
    # 方策π(a|s) を出力する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def showPolicy(self, env):
        print( "  Policy : π(a|s) -----------------------------------")
        for s in env.states:
            for a in self.actions:
                print( "    π('{0}'|'{1}') = {2:.3f}".format(a, s, self.pai[(s, a)]))

    #*********************************************************************
    # Agent ｸﾗｽ : showStateValues
    # 状態価値関数Ｖπ(s) を出力する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def showStateValues(self, env):
        print( "  State-value function : Ｖπ(s) ---------------------")
        for s in env.states:
            print( "    Ｖπ('{0}') = {1:.3f}".format(s, self.v_pais[s]))

    #*********************************************************************
    # Agent ｸﾗｽ : showActionValues
    # 行動価値関数Ｑπ(s,a) を出力する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def showActionValues(self, env):
        print( "  Action-value function : Ｑπ(s,a) ------------------")
        for s in env.states:
            for a in self.actions:
                print( "    Ｑπ('{0}','{1}') = {2:.3f}".format(s, a, self.q_paisa[(s,a)]))


#*************************************************************************
# 主制御
#*************************************************************************

# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの指定
epsilon = 0.2
gamma = 0.8
delta_thresh = 0.001
param = HyperParam(epsilon, gamma, delta_thresh)

# 環境のｲﾝｽﾀﾝｽを作成
env = Environment()

# ｴｰｼﾞｪﾝﾄのｲﾝｽﾀﾝｽを作成
agent = Agent(env)

# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを出力
param.showHyperParameters()

# 方策反復法による方策改善
agent.iteratePolicy(env, param)
