#*****************************************************************************
# ﾘｽﾄ11-(07)-2_価値反復法による方策改善
#*****************************************************************************
import sys
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline


#*************************************************************************
# ｸﾗｽ     Environment
#-------------------------------------------------------------------------
# 環境のｸﾗｽ
#*************************************************************************
class Environment:

    #*********************************************************************
    # Environment ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
    #---------------------------------------------------------------------
    # 以下のｲﾝｽﾀﾝｽ変数を定義する：
    #    states       ：状態値s（＝現在の位置座標(x,y)）の集合Ｓ
    #*********************************************************************
    def __init__(self):

        # 状態s の集合Ｓを、状態名を表す文字列のリストで作成
        self.states = ["s1", "s2"]

    #*********************************************************************
    # Environment ｸﾗｽ : transit
    # 状態s で行動a を取った時の状態遷移に伴う
    # （遷移確率Ｐss'a、報酬Ｒss'a、次の状態s'）のタプル型オブジェクトを
    # ﾘｽﾄで返す（これは(表3.2)を実装したもの）。
    #---------------------------------------------------------------------
    # 引数：
    #   self  ：Agent のｲﾝｽﾀﾝｽ
    #   s     ：現在の状態s
    #   a     ：状態s でとる行動a
    #---------------------------------------------------------------------
    # 戻り値：
    # (第１返値) 以下の値の組のタプル型ｵﾌﾞｼﾞｪｸﾄ(p,r,s) の配列
    #      p   : 状態s で行動a を取った時の状態s'への遷移確率Ｐss'a
    #      r   : 状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
    #      s'  : 状態s で行動a を取った時の遷移先の状態s'
    #*********************************************************************
    def transit(self, s, a):

        if( s == "s1" ):
            if( a == "a1" ):
                return [ (0.3,  1.0, "s1"), (0.7, -2.0, "s2") ]
            elif( a == "a2" ):
                return [ (0.5,  1.0, "s1"), (0.5,  2.0, "s2") ]

        elif( s == "s2" ):
            if( a == "a1" ):
                return [ (0.6, -1.0, "s1"), (0.4,  2.0, "s2") ]
            elif( a == "a2" ):
                return [ (0.7,  1.0, "s1"), (0.3,  1.0, "s2") ]

        return [ (1.0,  0.0,   s ) ]


#*************************************************************************
# ｸﾗｽ     HyperParam
#-------------------------------------------------------------------------
# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀのｸﾗｽ
#*************************************************************************
class HyperParam:

    #*********************************************************************
    # HyperParam ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
    #---------------------------------------------------------------------
    # 以下のｲﾝｽﾀﾝｽ変数を定義し、指定値で初期化する：
    #
    #    gamma        ：割引率γ (0≦γ＜1)
    #    delta_thresh ：状態価値関数Ｖπ(s) の更新量のしきい値
    #              (これより更新量が小さければ、状態価値関数の更新を終了する)
    #    iterate_max  ：価値反復の最大繰り返し回数
    #
    #   ----------------------------------------------------------------------
    #
    #*********************************************************************
    def __init__(self, gamma, delta_thresh, iterate_max ):

        # gamma：割引率γ
        self.gamma = gamma

        # delta_thresh ：状態価値関数Ｖπ(s) の更新量のしきい値
        #              （これより更新量が小さければ更新を終了する）
        self.delta_thresh = delta_thresh

        # iterate_max : 価値反復の最大繰り返し回数
        self.iterate_max = iterate_max



    #*********************************************************************
    # HyperParam ｸﾗｽ : showHyperParameters
    # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを出力する。
    #---------------------------------------------------------------------
    # 引数：無し
    #*********************************************************************
    def showHyperParameters(self):
        print( "Hyper parameters :")
        print( "  (1) γ={0:.3f}".format(self.gamma) )
        print( "  (2) delta_thresh={0}".format(self.delta_thresh) )
        print( "  (3) iterate_max={0}".format(self.iterate_max) )


#*************************************************************************
# クラス     Agent
#-------------------------------------------------------------------------
# ｴｰｼﾞｪﾝﾄのｸﾗｽ
# 価値反復法による方策改善：(表3.2) への適用
#*************************************************************************
class Agent:

    #*********************************************************************
    # Agent ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
    #---------------------------------------------------------------------
    # 以下のｲﾝｽﾀﾝｽ変数を定義し初期化する：
    #
    #   actions ：行動aの集合Ａを、行動名を表す文字列のリストで作成
    #   pai     ：方策πを辞書型で作成
    #                ｷｰ：(s,a)、ﾊﾞﾘｭｰ：π(a|s)
    #                方策の初期値：行動 "a1"を50％、"a2"を50％の割合で選択
    #   v_pais  ：状態s の状態価値関数Ｖπ(s)を辞書型で作成し、0で初期化
    #                ｷｰ：s(状態名を表す文字列型)、ﾊﾞﾘｭｰ：Ｖπ(s)
    #   q_paisa ：状態s での行動a の行動価値関数Ｑπ(s,a)を辞書型で作成し
    #             0で初期化
    #                ｷｰ：(s,a) のタプル型、ﾊﾞﾘｭｰ：Ｑπ(s,a)
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def __init__(self, env):

        # 行動aの集合Ａを、行動名を表す文字列のリストで作成
        self.actions = ["a1", "a2"]

        # 方策πを辞書型で作成
        #     ｷｰ：(s,a)、ﾊﾞﾘｭｰ：π(a|s)
        #     (実は初期化は不要だが、履歴表示の為に初期化しておく)
        self.pai = {}
        for s in env.states:
            for a in self.actions:
                self.pai[(s, a)] = 1.0/len(self.actions)

        # 状態(=位置)の状態価値関数Ｖπ(s)を辞書型で作成し、0で初期化
        # ｷｰ：s(位置：2次元座標を表すタプル型)、ﾊﾞﾘｭｰ：Ｖπ(s)
        self.v_pais = {}
        for s in env.states:
            self.v_pais[s] = 0

        # 状態s での行動a 行動価値関数Ｑπ(s,a)を辞書型で作成し、0で初期化
        # ｷｰ：(s,a) のタプル型、ﾊﾞﾘｭｰ：Ｑπ(s,a)
        #     (実は初期化は不要だが、履歴表示の為に初期化しておく)
        self.q_paisa = {}
        for s in env.states:
            for a in self.actions:
                self.q_paisa[(s,a)] = 0


    #*********************************************************************
    # Agent ｸﾗｽ : updatePolicy
    # 状態s での最適行動価値関数Ｑ*(s,a)を取得し、
    # 状態s での方策確率π(a |s) の改善を greedy法で行う。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #   param   ：HyperParam のｲﾝｽﾀﾝｽ
    #   s       ：状態s
    #---------------------------------------------------------------------
    # 戻り値：
    # (第１返値) q_aster : 状態s での最適行動価値関数Ｑ*(s,a)
    #*********************************************************************
    def updatePolicy(self, env, param, s):

        # 最適行動価値関数Ｑ*(s,a) を初期化
        q_aster = -sys.maxsize -1

        # 最適行動価値関数Ｑ*(s,a)を与える行動a* の集合Ａ*(s)をリストで作成
        a_asters = []

        # 全ての行動a について繰り返す
        for a in self.actions:

            # 状態s で行動a を取った時の状態遷移に伴う (p,r,s_dash) をタプル型オブジェクトで持ち、
            # その組をリスト(results) として取得する
            #     p＝Ｐss'a：状態がs の時に、行動a を選択して状態がs'に遷移する確率
            #     r＝Ｒss'a：状態がs の時に、行動a を選択して状態がs'に遷移した時の報酬
            #     s_dash＝s'：遷移先の状態s'
            results = env.transit(s, a)

            # 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) を計算する
            # q_paisa＝Ｑπ(s,a)＝Σs'Ｐss'a×｛Ｒss'a＋γＶπ(s')｝
            q_paisa = 0
            for p, r, s_dash in results:
                q_paisa += p * (r + param.gamma * self.v_pais[s_dash])
            self.q_paisa[(s,a)] = q_paisa

            # 状態s での最適行動価値関数Ｑ*(s,a)を更新し、
            # その時の行動a* も記録する。
            if q_paisa > q_aster:
                q_aster = q_paisa
                a_asters = [a]
            elif q_paisa == q_aster:
                a_asters.append(a)

        # M*：集合Ａ*(s)の要素数
        M_aster = len(a_asters)
        pai_aster = 1/M_aster
        pai_a     = 0

        # (greedy法による方策改善)
        # 着目している状態s での
        # (1) 行動a*            の方策確率π(a*|s)を 1/M*
        # (2) 他の行動a (a≠a*) の方策確率π(a |s)を 0
        # に各々更新する。
        for a in self.actions:
            if( a in a_asters ):
                self.pai[(s, a)] = pai_aster 
            else:
                self.pai[(s, a)] = pai_a

        # 状態s での最適行動価値関数Ｑ*(s,a)を返す
        return q_aster

    #*********************************************************************
    # Agent ｸﾗｽ : iterateValue
    # 価値反復法による方策改善を行う。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #   param   ：HyperParam のｲﾝｽﾀﾝｽ
    #---------------------------------------------------------------------
    # 戻り値：    なし
    #*********************************************************************
    def iterateValue(self, env, param):
        # 状態価値関数の更新履歴    
        vpiHistory = np.zeros((len(env.states), param.iterate_max))
        
        # 終了条件に達するまで繰り返し
        iterateNo = 0
        while True:
            # 繰り返し最大回数に達したか？
            iterateNo += 1
            if( iterateNo > param.iterate_max ):
                break

            print( "\n【 iterateValue : No.={0} 】".format(iterateNo) )

            # 方策π(a|s) を出力
            self.showPolicy(env)

            # 状態価値関数Ｖπ(s) を出力
            self.showStateValues(env)

            # 行動価値関数Ｑπ(s,a) を出力
            self.showActionValues(env)

            # 状態価値関数Ｖπ(s) を履歴に保存
            sii = 0
            for s in env.states:
                vpiHistory[sii, iterateNo-1] = self.v_pais[s]
                sii += 1

            # 状態s の全体に渡る最適状態価値関数Ｖ*(s)の変化の最大値
            delta_max = 0

            # 状態s についての繰り返し
            for s in env.states:

                # 状態s での最適行動価値関数Ｑ*(s,a)を得て、
                # 状態s について方策改善を行う。
                # 価値反復法では、Ｑ*(s,a)が、状態s での最適状態価値関数Ｖ*(s)になる
                q_aster = self.updatePolicy(env, param, s)
                v_aster = q_aster

                # 状態s での最適状態価値関数Ｖ*(s)の変化分を計算する
                delta_max = max(delta_max, abs(self.v_pais[s] - v_aster))

                # 状態s での最適状態価値関数Ｖ*(s)を状態価値関数Ｖ(s)として保存する
                self.v_pais[s] = v_aster

            # 状態s の全体に渡る最適状態価値関数Ｖ*(s)の変化の最大値が
            # しきい値より小さい場合、反復を終了する
            if delta_max < param.delta_thresh:
                break

        # 最適状態価値関数Ｖ*(s)の履歴のｸﾞﾗﾌ表示 
        self.showVpiHistory(env, iterateNo, vpiHistory)

    #*********************************************************************
    # Agent ｸﾗｽ : showPolicy
    # 方策π(a|s) を出力する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def showPolicy(self, env):
        print( "  Policy : π(a|s) -----------------------------------")
        for s in env.states:
            for a in self.actions:
                print( "    π('{0}'|'{1}') = {2:.3f}".format(a, s, self.pai[(s, a)]))

    #*********************************************************************
    # Agent ｸﾗｽ : showStateValues
    # 状態価値関数Ｖπ(s) を出力する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def showStateValues(self, env):
        print( "  State-value function : Ｖπ(s) ---------------------")
        for s in env.states:
            print( "    Ｖπ('{0}') = {1:.3f}".format(s, self.v_pais[s]))

    #*********************************************************************
    # Agent ｸﾗｽ : showActionValues
    # 行動価値関数Ｑπ(s,a) を出力する。
    #---------------------------------------------------------------------
    # 引数：
    #   env     ：Environment のｲﾝｽﾀﾝｽ
    #*********************************************************************
    def showActionValues(self, env):
        print( "  Action-value function : Ｑπ(s,a) ------------------")
        for s in env.states:
            for a in self.actions:
                print( "    Ｑπ('{0}','{1}') = {2:.3f}".format(s, a, self.q_paisa[(s,a)]))

    #*********************************************************************
    # Agent ｸﾗｽ : showVpiHistory
    # 最適状態価値関数Ｖ*(s)（＝状態価値関数Ｖπ(s)）の
    # 価値反復に伴う履歴のｸﾞﾗﾌ表示 
    #---------------------------------------------------------------------
    # 引数：
    #   env          : Environment のｲﾝｽﾀﾝｽ
    #   lastStep     : 繰り返しの最終ｽﾃｯﾌﾟ番号
    #   vpiHistory[] : 状態価値関数Ｖπ(s)のｽﾃｯﾌﾟ毎の履歴 
    #*********************************************************************
    def showVpiHistory(self, env, lastStep, vpiHistory):
        # 表示範囲 (X)
        xlist = np.zeros((lastStep))
        for i in range(lastStep):
            xlist[i] = i

        # 表示範囲 (Y)
        yrange = np.max(vpiHistory[:,:]) - np.min(vpiHistory[:,:])
        ymin = np.min(vpiHistory[:,:]) - yrange * 0.1
        ymax = np.max(vpiHistory[:,:]) + yrange * 0.1

        # ｸﾞﾗﾌ表示      
        plt.figure(figsize=(15, 3))
        plt.title("history of Vpai(s)")
        
        # 状態s についての繰り返し
        dispColors = ['black', 'cornflowerblue']
        dispStyles = ['-', ':']
        sii = 0
        for s in env.states:
            plt.plot(xlist, vpiHistory[sii,:lastStep], marker='o', linestyle=dispStyles[sii%2],
                markeredgecolor='black', color=dispColors[sii%2], label='Vpai [{0}]'.format(s))
            sii += 1

        plt.xlim(0, lastStep-1)
        plt.ylim(ymin, ymax)
        plt.xlabel("step")
        plt.ylabel("Value function")
        plt.legend(loc='lower right')
        plt.grid(True)
        plt.show()


#*************************************************************************
# 主制御
#*************************************************************************

# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの指定
gamma = 0.8
delta_thresh = 0.001
iterate_max = 100
param = HyperParam(gamma, delta_thresh, iterate_max)

# 環境のｲﾝｽﾀﾝｽを作成
env = Environment()

# ｴｰｼﾞｪﾝﾄのｲﾝｽﾀﾝｽを作成
agent = Agent(env)

# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを出力
param.showHyperParameters()

# 価値反復法による方策改善
agent.iterateValue(env, param)
