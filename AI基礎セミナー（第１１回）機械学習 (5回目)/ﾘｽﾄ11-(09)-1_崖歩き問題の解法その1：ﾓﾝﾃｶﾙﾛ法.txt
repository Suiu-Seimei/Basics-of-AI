#*************************************************************************
#  ﾘｽﾄ11-(09)-1_崖歩き問題の解法その1：ﾓﾝﾃｶﾙﾛ法
#  ﾓﾝﾃｶﾙﾛ法で、
# 「(8.1) ﾓﾃﾞﾙﾌﾘｰな手法での例題」の「(例題) 崖歩き」の決定論的方策πを作成する。
#  ※ 事前に「ﾘｽﾄ11-(08)-1_崖歩き問題の環境と表示のｸﾗｽ」を実行しておくこと。
#*************************************************************************
#import numpy as np
import time
#import matplotlib.pyplot as plt


#*************************************************************************
# ｸﾗｽ     HyperParam
#-------------------------------------------------------------------------
# ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀのｸﾗｽ
#*************************************************************************
class HyperParam:

  #***********************************************************************
  # HyperParam ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し、指定値で初期化する：
  #
  # eps_First： ｻﾝﾌﾟﾘﾝｸﾞ時のε-greedy法のｰﾊﾟﾗﾒｰﾀ (0≦ε≦1)εの初期値
  # eps_Last ： 学習時に、ε-greedy法のﾊﾟﾗﾒｰﾀεを徐々に減少させながら、
  #             ｴﾋﾟｿｰﾄﾞを繰り返して行動価値関数Ｑπ(s,a) の精度を上げてゆく。
  #             eps_Last は学習のためのｴﾋﾟｿｰﾄﾞ繰り返しの終了判断のεの最終値
  # eps_Rate ：εの1ｴﾋﾟｿｰﾄﾞ毎の減少率(0＜ε＜1、ε←⊿εrate×ε)
  # eps_Run  ：εの実行時の値
  #          （ε=0 で実行すると、方策πが完成していなくて終端に到達しない場合がある。
  #            εを 0 より大きいができるだけ小さい値で実行することにより、
  #            学習済の方策πを最大限に生かす方針の下で開始点から終了点まで移動する。)
  # trace    ：True:追跡出力する、False:しない
  #
  #***********************************************************************
  def __init__(self, eps_First, eps_Last, eps_Rate, eps_Run, trace):
    self.eps_First = eps_First
    self.eps_Last = eps_Last
    self.eps_Rate = eps_Rate
    self.eps_Run = eps_Run
    self.trace = trace


#*************************************************************************
# ｸﾗｽ     Agent
#-------------------------------------------------------------------------
# 崖歩きのｴｰｼﾞｪﾝﾄのｸﾗｽ
#*************************************************************************
class Agent:

  #***********************************************************************
  # Agent ｸﾗｽ : ｺﾝｽﾄﾗｸﾀ
  #-----------------------------------------------------------------------
  # 以下のｲﾝｽﾀﾝｽ変数を定義し初期化する：
  #
  #    env           ：Environment のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  #    actions       ：取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａ
  #    policy        ：決定論的方策π(s)（＝現在の状態s でとる行動a）
  #    q             ：状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
  #    n             ：状態s で行動a を取った訪問回数
  #
  #***********************************************************************
  def __init__(self, env):
    
    # 取りうる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）の集合Ａをﾀﾌﾟﾙ型(Δx,Δy)の配列として保持する
    # (0,-1)：上, (-1,0)：左, (1,0)：右, (0,1)：下
    self.actions = [(0, -1), (-1, 0), (1, 0), (0, 1)]

    # 決定論的方策π を辞書型で保持し、乱数で初期化する
    #  キー：s、値：a
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    self.policy = {}
    for s in env.states:
      self.policy[s] = self.actions[np.random.randint(len(self.actions))]

    # 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) "q" を
    # ディクショナリ型で保持し、計算上あり得ない小さい値で初期化する 
    #  キー：(s,a)、値：q
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    #     q： 状態s で行動a を取った時の行動価値関数Ｑπ(s,a) 
    self.q = {}
    for s in env.states:
      for a in self.actions:
        self.q[(s, a)] = -10**10

    # 状態s で行動a を取った訪問回数 "n" をディクショナリ型で保持し、0で初期化する 
    #  キー：(s,a)、値：n
    #     s： 状態s（＝現在の位置座標(x,y)）
    #     a： 状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
    #     n： 状態s で行動a を取った訪問回数Ｎ(s,a)
    self.n = {}
    for s in env.states:
      for a in self.actions:
        self.n[(s, a)] = 0

  #***********************************************************************
  # Agent ｸﾗｽ : getEpisode
  # ｼﾐｭﾚｰｼｮﾝにより、1回分のｴﾋﾟｿｰﾄﾞを取得する
  #-----------------------------------------------------------------------
  # 引数：
  #   env     ：Environment のｲﾝｽﾀﾝｽ
  #   epsilon ：ε-greedy法のﾊﾟﾗﾒｰﾀε
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 
  #             開始点から出口に至るまでの１回分のｴﾋﾟｿｰﾄﾞの
  #             状態遷移の流れを、ﾀﾌﾟﾙ型オブジェクト (s,a,r)  の
  #             ﾘｽﾄとして保存したもの
  #
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #          r：状態s で行動a を取った時の状態s'への遷移に伴う報酬Ｒss'a
  #***********************************************************************
  def getEpisode(self, env, epsilon):
    # １回分のｴﾋﾟｿｰﾄﾞの状態遷移の流れをﾘｽﾄで持つ
    episode = []

    # 状態s の初期値＝開始点
    s = env.state_ini

    # 「終端状態」となるまで繰り返す
    while True:
      if np.random.random() < epsilon:
        # εの割合で、ランダムに行動a を選択する
        a = self.actions[np.random.randint(len(agent.actions))]
      else:
        # 1-εの割合で、決定論的方策πに従って行動a を選択する
        a = self.policy[s]

      # 現在の状態s で、上記で選択した行動a を取り、状態遷移を行う
      r, s_dash = env.transit(s, a)
      
      # ｴﾋﾟｿｰﾄﾞへ状態遷移情報を追加する
      episode.append((s, a, r))
      
      # 状態遷移で終点に着いたら「終端状態」となるので、本ｴﾋﾟｿｰﾄﾞは終了する
      x, y = s_dash
      if env.cliff[y][x] == 'G':
        break

      # 状態s を更新
      s = s_dash

    return episode

  #***********************************************************************
  # Agent ｸﾗｽ : updatePolicy
  # 方策改善 (最適のものを１つだけ採用する)
  #-----------------------------------------------------------------------
  # 指定した状態s における最適行動価値関数Ｑ*(s,a) を求め(=q_max)、
  # その時の行動a* (=a_best)を、状態sにおける最適方策π*(s) として
  # 行動方策π(s) "policy[s]"を更新する。
  #          s：状態s (＝位置座標(x,y)）
  #          a：状態s でとる行動a（＝移動ﾍﾞｸﾄﾙ(Δx,Δy)）
  #-----------------------------------------------------------------------
  # 引数：
  #   s    ： 現在の状態s (＝位置座標(x,y))
  #***********************************************************************
  def updatePolicy(self, s):
    q_max = -10**10
    a_best = None
    for a in self.actions:
      if self.q[(s, a)] > q_max:
        q_max = self.q[(s, a)]
        a_best = a

    self.policy[s] = a_best

  #***********************************************************************
  # Agent ｸﾗｽ : train
  # ｼﾐｭﾚｰｼｮﾝにより、
  # ε-greedy法のﾊﾟﾗﾒｰﾀεを、ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの現象率で徐々に減少させ、
  # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの指定値より小さくなったら学習を終了するまで、
  # ｴﾋﾟｿｰﾄﾞを取得し、状態価値関数Ｖπ(s) を見積もる。
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode_lengths: 
  #      1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数を要素とするﾘｽﾄ型で保持し、その履歴とする
  #***********************************************************************
  def train(self, env, param):
    # 経過時間測定開始
    t1 = time.time()

    # 1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をﾘｽﾄ型で保持する
    episode_lengths = []

    # ε-greedy法のﾊﾟﾗﾒｰﾀεの初期化
    epsilon = param.eps_First

    # ε-greedy法のﾊﾟﾗﾒｰﾀεを徐々に減少させながら、
    # ｴﾋﾟｿｰﾄﾞの取得とそれによる方策改善を繰り返す
    seqNo = 0
    while True:
      seqNo += 1

      # 1回分のｴﾋﾟｿｰﾄﾞを取得
      episode = self.getEpisode(env, epsilon)

      # 1回分のｴﾋﾟｿｰﾄﾞでの状態遷移数を追加
      episode_lengths.append(len(episode))

      # 各状態s での決定論的方策π(s)のｵﾝﾎﾟﾘｼｰの範囲を確認するために
      # 終端から開始点に向かって参照できるように逆順にする
      episode.reverse()

      # 終端から開始点に向かって状態遷移情報を参照し、
      # 各状態s での決定論的方策π(s)のｵﾝﾎﾟﾘｼｰの範囲内の状態遷移情報で、
      # 以下の一連の処理で方策改善を行う

      # total_r は最終値は総報酬であるが、各状態s での総報酬にもなる
      total_r = 0
      
      last = False
      for (s, a, r) in episode:

        # 行動a が状態s での方策π(s)でなかったら
        # 本ｴﾋﾟｿｰﾄﾞによる方策改善で、残りの状態遷移を遡って参照することはしない。
        # 最後に参照する状態遷移情報は、
        # ε-greedy法によりεの割合で既存の方策πでない方策π'での行動a' 
        # を取ったことによる状態遷移情報
        if a != self.policy[s]:
          last = True

        # 各状態(s, a)での総報酬を更新
        total_r += r
      
        # 状態(s, a)への訪問回数を更新する
        self.n[(s, a)] += 1

        # 状態s の行動価値関数Ｑπ(s,a) を報酬Ｒss'a の総和の平均値として算出する
        self.q[(s, a)] += (total_r - self.q[(s, a)]) / self.n[(s, a)]

        # 状態s での方策を改善する
        self.updatePolicy(s)

        # 残りの状態遷移がｵﾝﾎﾟﾘｼｰの範囲内でなくなったら終了
        if last:
          break

      # ε-greedy法のﾊﾟﾗﾒｰﾀεを、ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの現象率で徐々に減少させ、
      # ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀの指定値より小さくなったら学習を終了する
      epsilon *= param.eps_Rate
      if epsilon < param.eps_Last:
        # 方策改善最後の1ｴﾋﾟｿｰﾄﾞ分の足跡を、表示する
        if(param.trace == True):
          print("\ntrain: last episode seqNo={0}".format(seqNo))
          showFootprint(self, env, episode)
        break

      # 1ｴﾋﾟｿｰﾄﾞ分の足跡を、100回ごとに表示する
      if(param.trace == True) and (seqNo % 100 == 0):
        print("\ntrain: episode seqNo={0}".format(seqNo))
        showFootprint(self, env, episode)

    # 経過時間を表示する
    t2 = time.time()
    print("学習経過時間：{0:.3f} [秒]".format(t2-t1))

    return episode_lengths

  #***********************************************************************
  # Agent ｸﾗｽ : run
  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路探索)を取得する
  #-----------------------------------------------------------------------
  # 引数：
  #   env   ：Environment のｲﾝｽﾀﾝｽ
  #   param ：HyperParam のｲﾝｽﾀﾝｽ
  #-----------------------------------------------------------------------
  # 戻り値：
  # (第１返値) episode: 1ｴﾋﾟｿｰﾄﾞ分の状態遷移
  #***********************************************************************
  def run(self, env, param):

    # getEpisode 関数を、実行時のεで実行
    episode = self.getEpisode(env, param.eps_Run)
    return episode


#*************************************************************************
# 主制御
#*************************************************************************

#-------------------------------------------------------------------------
# (1) ﾊｲﾊﾟｰﾊﾟﾗﾒｰﾀを設定
#-------------------------------------------------------------------------
eps_First = 0.9
eps_Last = 0.01
eps_Rate = 0.999
eps_Run = 0.001
param = HyperParam(eps_First, eps_Last, eps_Rate, eps_Run, False)

#-------------------------------------------------------------------------
# (2) 環境のｲﾝｽﾀﾝｽを作成
#-------------------------------------------------------------------------
env = Environment()

#-------------------------------------------------------------------------
# (3) 方策πの学習と利用を、100回繰り返す
#-------------------------------------------------------------------------
showObj = ShowFunc()
for rii in range(0,100):

  # Agent ｸﾗｽのｲﾝｽﾀﾝｽを作成する
  agent = Agent(env)

  # ﾓﾝﾃｶﾙﾛ法で方策πを学習する
  param.trace = False
  episode_lengths = agent.train(env, param)

  # 方策πの学習で、1ｴﾋﾟｿｰﾄﾞ毎の状態遷移数の履歴をｸﾞﾗﾌ表示する
  showObj.showLearningCurve(episode_lengths)

  # 方策πの学習終了時の、行動価値関数Ｑπ(s,a)を表示する
  showObj.showQ(agent, env)

  # 学習済の方策で1ｴﾋﾟｿｰﾄﾞ分(開始点から出口に至る経路)を取得する
  param.trace = True
  episode = agent.run(env, param)

  # 1ｴﾋﾟｿｰﾄﾞ分の足跡を表示する
  showObj.showFootprint(agent, env, episode)
